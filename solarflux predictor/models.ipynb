{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Example values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data(lag = 27, reshape = True):\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df.drop('Unnamed: 0', inplace = True, axis = 1)\n",
    "    df.columns = ['Date', 'sfu']\n",
    "    df.Date = pd.to_datetime(df.Date)\n",
    "    train_data = df[(df['Date'].dt.year < 2003) & (df['Date'].dt.year > 1979)]\n",
    "    test_data = df[(df['Date'].dt.year >= 2003) & (df['Date'].dt.year <= 2014)]    \n",
    "    train_X, train_y = split_sequence(np.array(train_data['sfu']), lag)\n",
    "    test_X, test_y = split_sequence(np.array(test_data['sfu']), lag)\n",
    "    if reshape:    \n",
    "        train_X = train_X.reshape((train_X.shape[0], lag, 1))\n",
    "        test_X = test_X.reshape((test_X.shape[0], lag, 1))\n",
    "    return train_X, train_y, test_X, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data_CNN(lag = 27, reshape = True):\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df.drop('Unnamed: 0', inplace = True, axis = 1)\n",
    "    df.columns = ['Date', 'sfu']\n",
    "    df.Date = pd.to_datetime(df.Date)\n",
    "    train_data = df[(df['Date'].dt.year < 2003) & (df['Date'].dt.year > 1979)]\n",
    "    test_data = df[(df['Date'].dt.year >= 2003) & (df['Date'].dt.year <= 2014)]    \n",
    "    train_X, train_y = split_sequence(np.array(train_data['sfu']), lag)\n",
    "    test_X, test_y = split_sequence(np.array(test_data['sfu']), lag)\n",
    "    if reshape:    \n",
    "        train_X = train_X.reshape((train_X.shape[0],1, lag, 1))\n",
    "        test_X = test_X.reshape((test_X.shape[0],1, lag, 1))\n",
    "    return train_X, train_y, test_X, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_sequences_multistep(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(n_steps_in, len(sequences)-n_steps_out):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i-n_steps_in:i], sequences[i: i+ n_steps_out]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_multistep(in_steps, out_steps, reshape = True):\n",
    "    X, y = list(), list()\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df.drop('Unnamed: 0', inplace = True, axis = 1)\n",
    "    df.columns = ['Date', 'sfu']\n",
    "    df.Date = pd.to_datetime(df.Date)\n",
    "    train_data = df[(df['Date'].dt.year < 2003) & (df['Date'].dt.year > 1979)]\n",
    "    test_data = df[(df['Date'].dt.year >= 2003) & (df['Date'].dt.year <= 2014)]    \n",
    "    train_X, train_y = split_sequences_multistep(np.array(train_data['sfu']), in_steps, out_steps)\n",
    "    test_X, test_y = split_sequences_multistep(np.array(test_data['sfu']), in_steps, out_steps)\n",
    "    if reshape:    \n",
    "        train_X = train_X.reshape((train_X.shape[0],in_steps,1))\n",
    "        test_X = test_X.reshape((test_X.shape[0], in_steps, 1))\n",
    "    return train_X, train_y, test_X, test_y\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN_data_multistep(in_steps, out_steps, reshape = True):\n",
    "    X, y = list(), list()\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df.drop('Unnamed: 0', inplace = True, axis = 1)\n",
    "    df.columns = ['Date', 'sfu']\n",
    "    df.Date = pd.to_datetime(df.Date)\n",
    "    train_data = df[(df['Date'].dt.year < 2003) & (df['Date'].dt.year > 1979)]\n",
    "    test_data = df[(df['Date'].dt.year >= 2003) & (df['Date'].dt.year <= 2014)]    \n",
    "    train_X, train_y = split_sequences_multistep(np.array(train_data['sfu']), in_steps, out_steps)\n",
    "    test_X, test_y = split_sequences_multistep(np.array(test_data['sfu']), in_steps, out_steps)\n",
    "    if reshape:    \n",
    "        train_X = train_X.reshape((train_X.shape[0], 1, in_steps,1))\n",
    "        test_X = test_X.reshape((test_X.shape[0], 1, in_steps, 1))\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, MaxPooling1D, Flatten, Conv1D, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.model_root = 'models/'\n",
    "        self.model_name = 'MLP.h5'\n",
    "        self.model_path = self.model_root + self.model_name\n",
    "        \n",
    "    def build_model(self, lag):\n",
    "        model = model = Sequential()\n",
    "        model.add(Dense(128, activation = \"relu\"))\n",
    "        model.add(Dense(128, activation = \"relu\"))\n",
    "        model.add(Dense(128, activation = \"relu\"))\n",
    "        model.add(Dense(128, activation = \"relu\"))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        self.model = model\n",
    "        print(\"MLP moedel built!\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self,train_X, train_y, test_X, test_y, verbose = 1, epochs = 100, lag = 27):\n",
    "        if self.model == None:\n",
    "          model =  self.build_model(lag)\n",
    "        checkpoint = ModelCheckpoint(self.model_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "        model.fit(train_X, train_y, epochs = epochs, verbose = verbose, validation_data = (test_X, test_y), callbacks=[checkpoint])\n",
    "        self.model = model\n",
    "          \n",
    "    def evaluate(self, test_X, test_y):\n",
    "        model = self.model\n",
    "        model.load_weights(self.model_path)\n",
    "        # Assuming you have your true values in 'y_true' and predicted values in 'y_pred'\n",
    "        pred_y = model.predict(test_X)\n",
    "        r2 = r2_score(test_y, pred_y)\n",
    "        rmse = np.sqrt(mean_squared_error(test_y, pred_y))\n",
    "        print(\"R2 score:\", r2)\n",
    "        print(\"RMSE: \", rmse)        \n",
    "        return r2, rmse, pred_y\n",
    "\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.plot()\n",
    "          \n",
    "          \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/MLP.h5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"models/\" + \"MLP.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP moedel built!\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/262 [==============================] - 1s 3ms/step - loss: 452.4427 - val_loss: 23.9174\n",
      "Epoch 2/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 64.3426 - val_loss: 38.5461\n",
      "Epoch 3/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 50.1889 - val_loss: 17.2655\n",
      "Epoch 4/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 45.9709 - val_loss: 18.1932\n",
      "Epoch 5/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 42.2452 - val_loss: 16.7748\n",
      "Epoch 6/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 41.5870 - val_loss: 16.2778\n",
      "Epoch 7/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 49.1569 - val_loss: 19.5570\n",
      "Epoch 8/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 35.7722 - val_loss: 34.8033\n",
      "Epoch 9/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 39.9930 - val_loss: 15.2340\n",
      "Epoch 10/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 38.2041 - val_loss: 20.6802\n",
      "Epoch 11/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 38.4286 - val_loss: 31.5197\n",
      "Epoch 12/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 36.0767 - val_loss: 14.1552\n",
      "Epoch 13/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 30.9034 - val_loss: 14.7513\n",
      "Epoch 14/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 39.7622 - val_loss: 20.4244\n",
      "Epoch 15/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 35.7939 - val_loss: 14.0167\n",
      "Epoch 16/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 32.0255 - val_loss: 14.3421\n",
      "Epoch 17/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 31.8684 - val_loss: 14.9945\n",
      "Epoch 18/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 30.5949 - val_loss: 16.0113\n",
      "Epoch 19/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 39.0312 - val_loss: 15.0862\n",
      "Epoch 20/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 37.1976 - val_loss: 23.0869\n",
      "Epoch 21/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 33.5403 - val_loss: 18.9037\n",
      "Epoch 22/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 31.6276 - val_loss: 17.6461\n",
      "Epoch 23/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 28.8949 - val_loss: 14.5160\n",
      "Epoch 24/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.7512 - val_loss: 18.5757\n",
      "Epoch 25/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 31.4225 - val_loss: 20.2341\n",
      "Epoch 26/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 30.2971 - val_loss: 55.6937\n",
      "Epoch 27/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 29.3992 - val_loss: 16.9872\n",
      "Epoch 28/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 31.9355 - val_loss: 14.5179\n",
      "Epoch 29/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.2067 - val_loss: 17.2575\n",
      "Epoch 30/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 39.1195 - val_loss: 31.2086\n",
      "Epoch 31/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 26.8403 - val_loss: 19.7487\n",
      "Epoch 32/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.4668 - val_loss: 14.0409\n",
      "Epoch 33/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 31.5882 - val_loss: 14.4247\n",
      "Epoch 34/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 30.3824 - val_loss: 14.0273\n",
      "Epoch 35/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 31.1682 - val_loss: 15.1116\n",
      "Epoch 36/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 38.1796 - val_loss: 15.9363\n",
      "Epoch 37/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.2226 - val_loss: 17.1386\n",
      "Epoch 38/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 28.6950 - val_loss: 15.5208\n",
      "Epoch 39/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 31.1937 - val_loss: 20.6885\n",
      "Epoch 40/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 28.6389 - val_loss: 14.1549\n",
      "Epoch 41/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 30.3433 - val_loss: 13.9546\n",
      "Epoch 42/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 28.3664 - val_loss: 15.3483\n",
      "Epoch 43/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.3906 - val_loss: 24.3883\n",
      "Epoch 44/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.4755 - val_loss: 13.8192\n",
      "Epoch 45/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.4489 - val_loss: 17.4588\n",
      "Epoch 46/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 28.3087 - val_loss: 20.7110\n",
      "Epoch 47/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 29.2126 - val_loss: 14.9090\n",
      "Epoch 48/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.9277 - val_loss: 15.1800\n",
      "Epoch 49/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 35.0904 - val_loss: 14.8513\n",
      "Epoch 50/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.8323 - val_loss: 15.1163\n",
      "Epoch 51/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 26.7385 - val_loss: 14.2301\n",
      "Epoch 52/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 27.1161 - val_loss: 16.9216\n",
      "Epoch 53/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 29.1491 - val_loss: 18.1367\n",
      "Epoch 54/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.1935 - val_loss: 15.0335\n",
      "Epoch 55/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.9807 - val_loss: 15.1203\n",
      "Epoch 56/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.6850 - val_loss: 13.9332\n",
      "Epoch 57/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 26.5612 - val_loss: 15.4867\n",
      "Epoch 58/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.5920 - val_loss: 24.4077\n",
      "Epoch 59/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 29.8061 - val_loss: 20.0650\n",
      "Epoch 60/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.9785 - val_loss: 13.9062\n",
      "Epoch 61/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.2561 - val_loss: 15.1695\n",
      "Epoch 62/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 26.6563 - val_loss: 17.9473\n",
      "Epoch 63/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 30.3174 - val_loss: 15.4421\n",
      "Epoch 64/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.8270 - val_loss: 15.4558\n",
      "Epoch 65/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.2704 - val_loss: 14.3597\n",
      "Epoch 66/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.3915 - val_loss: 16.0219\n",
      "Epoch 67/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.6992 - val_loss: 14.6598\n",
      "Epoch 68/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.8815 - val_loss: 16.8219\n",
      "Epoch 69/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.7182 - val_loss: 14.8242\n",
      "Epoch 70/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 25.8730 - val_loss: 22.2900\n",
      "Epoch 71/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.5261 - val_loss: 15.8224\n",
      "Epoch 72/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.5534 - val_loss: 20.6729\n",
      "Epoch 73/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.2761 - val_loss: 15.3167\n",
      "Epoch 74/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.6923 - val_loss: 14.6874\n",
      "Epoch 75/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.8211 - val_loss: 18.2114\n",
      "Epoch 76/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 26.4375 - val_loss: 14.2349\n",
      "Epoch 77/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.3752 - val_loss: 22.4095\n",
      "Epoch 78/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.3799 - val_loss: 15.4197\n",
      "Epoch 79/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.4961 - val_loss: 13.9990\n",
      "Epoch 80/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.1094 - val_loss: 14.0528\n",
      "Epoch 81/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.5014 - val_loss: 19.2005\n",
      "Epoch 82/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.1981 - val_loss: 16.3561\n",
      "Epoch 83/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 26.2926 - val_loss: 14.5443\n",
      "Epoch 84/100\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 30.0284 - val_loss: 14.1358\n",
      "Epoch 85/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.6125 - val_loss: 15.6597\n",
      "Epoch 86/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.3377 - val_loss: 15.0519\n",
      "Epoch 87/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.8914 - val_loss: 16.8618\n",
      "Epoch 88/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 28.0536 - val_loss: 14.4702\n",
      "Epoch 89/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 25.0964 - val_loss: 16.7944\n",
      "Epoch 90/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.3254 - val_loss: 22.0102\n",
      "Epoch 91/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 29.7672 - val_loss: 22.7699\n",
      "Epoch 92/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.7410 - val_loss: 15.2862\n",
      "Epoch 93/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 25.7314 - val_loss: 18.6556\n",
      "Epoch 94/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.9234 - val_loss: 17.2056\n",
      "Epoch 95/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.3128 - val_loss: 14.4470\n",
      "Epoch 96/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.4262 - val_loss: 14.2071\n",
      "Epoch 97/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 26.5047 - val_loss: 14.2953\n",
      "Epoch 98/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 31.9265 - val_loss: 14.1636\n",
      "Epoch 99/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 25.5898 - val_loss: 14.1295\n",
      "Epoch 100/100\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 27.8736 - val_loss: 17.5211\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "train_X, train_y, test_X, test_y = get_data(27, reshape = False)\n",
    "mlp.train(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step\n",
      "R2 score: 0.9838297092995881\n",
      "RMSE:  3.7174148630926336\n"
     ]
    }
   ],
   "source": [
    "r2, rmse, y_preds = mlp.evaluate(test_X = test_X, test_y = test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, MaxPooling1D, Flatten, Conv1D, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "class MLP_multistep():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.model_root = 'models/'\n",
    "        self.model_name = 'MLP-multisteps.h5'\n",
    "        self.model_path = self.model_root + self.model_name\n",
    "        \n",
    "    def build_model(self, lag, out_steps):\n",
    "        model = model = Sequential()\n",
    "        model.add(Dense(128, activation = \"relu\", input_shape = (27, 1)))\n",
    "        model.add(Dense(128, activation = \"relu\"))\n",
    "        model.add(Dense(128, activation = \"relu\"))\n",
    "        model.add(Dense(128, activation = \"relu\"))\n",
    "        model.add(Dense(out_steps))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        self.model = model\n",
    "        print(\"MLP moedel built!\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self,train_X, train_y, test_X, test_y, verbose = 1, epochs = 100, in_steps = 27, out_steps = 7):\n",
    "        if self.model == None:\n",
    "          model =  self.build_model(in_steps, out_steps)\n",
    "        checkpoint = ModelCheckpoint(self.model_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "        model.fit(train_X, train_y, epochs = epochs, verbose = verbose, validation_data = (test_X, test_y), callbacks=[checkpoint])\n",
    "        self.model = model\n",
    "          \n",
    "    def evaluate(self, test_X, test_y):\n",
    "        model = self.model\n",
    "        model.load_weights(self.model_path)\n",
    "        # Assuming you have your true values in 'y_true' and predicted values in 'y_pred'\n",
    "        pred_y = model.predict(test_X)\n",
    "        RMSE_scores = []\n",
    "        R2_scores = []    \n",
    "        for i in range(test_y.shape[1]):\n",
    "            mse = mean_squared_error(test_y[:, i], pred_y[:,  i])\n",
    "            rmse = np.sqrt(mse)\n",
    "            RMSE_scores.append(rmse)\n",
    "            \n",
    "        for i in range(test_y.shape[1]):\n",
    "            r2 = r2_score(test_y[:, i], pred_y[:,  i])        \n",
    "            R2_scores.append(r2)\n",
    "            \n",
    "        total_score = 0\n",
    "        for row in range(test_y.shape[0]):\n",
    "            for col in range(pred_y.shape[1]):\n",
    "                total_score += (test_y[row, col] - pred_y[row, col]**2)\n",
    "                total_score = np.sqrt(total_score/(test_y.shape[0] * test_y.shape[1]))\n",
    "                \n",
    "        return RMSE_scores, R2_scores, total_score, pred_y\n",
    "\n",
    "        \n",
    "    def plot(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP moedel built!\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 1486, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 7 and 3 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_22/dense_55/BiasAdd, IteratorGetNext:1)' with input shapes: [?,27,7], [?,3].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Swaroop\\solarflux predictor\\models.ipynb Cell 12\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mlp \u001b[39m=\u001b[39m MLP_multistep()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_X, train_y, test_X, test_y \u001b[39m=\u001b[39m get_data_multistep(\u001b[39m27\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mlp\u001b[39m.\u001b[39;49mtrain(train_X, train_y, test_X, test_y)\n",
      "\u001b[1;32mc:\\Users\\Swaroop\\solarflux predictor\\models.ipynb Cell 12\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X30sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m   model \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_model(in_steps, out_steps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X30sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_path, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X30sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_X, train_y, epochs \u001b[39m=\u001b[39;49m epochs, verbose \u001b[39m=\u001b[39;49m verbose, validation_data \u001b[39m=\u001b[39;49m (test_X, test_y), callbacks\u001b[39m=\u001b[39;49m[checkpoint])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X30sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9tcjo1_q.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 1486, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 7 and 3 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_22/dense_55/BiasAdd, IteratorGetNext:1)' with input shapes: [?,27,7], [?,3].\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP_multistep()\n",
    "train_X, train_y, test_X, test_y = get_data_multistep(27,3)\n",
    "mlp.train(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8346, 27, 1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x254b49d8670>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_preds)\n",
    "plt.plot(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, MaxPooling1D, Flatten, Conv1D, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "class LSTM2():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.model_root = 'models/'\n",
    "        self.model_name = 'LSTM2.h5'\n",
    "        self.model_path = self.model_root + self.model_name\n",
    "        \n",
    "    def build_model(self, lag):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, activation='relu', return_sequences= True, input_shape = (lag, 1)))\n",
    "        model.add(LSTM(32, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        self.model = model\n",
    "        print(\"MLP moedel built!\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self,train_X, train_y, test_X, test_y, verbose = 1, epochs = 100, lag = 27):\n",
    "        if self.model == None:\n",
    "          model =  self.build_model(lag)\n",
    "        checkpoint = ModelCheckpoint(self.model_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "        model.fit(train_X, train_y, epochs = epochs, verbose = verbose, validation_data = (test_X, test_y), callbacks=[checkpoint])\n",
    "        self.model = model\n",
    "          \n",
    "    def evaluate(self, test_X, test_y):\n",
    "        model = self.model\n",
    "        model.load_weights(self.model_path)\n",
    "        # Assuming you have your true values in 'y_true' and predicted values in 'y_pred'\n",
    "        pred_y = model.predict(test_X)\n",
    "        r2 = r2_score(test_y, pred_y)\n",
    "        rmse = np.sqrt(mean_squared_error(test_y, pred_y))\n",
    "        print(\"R2 score:\", r2)\n",
    "        print(\"RMSE: \", rmse)        \n",
    "        return r2, rmse, pred_y\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.plot()\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP moedel built!\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/262 [==============================] - 7s 21ms/step - loss: 2902.7527 - val_loss: 64.1613\n",
      "Epoch 2/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 132.8681 - val_loss: 59.0096\n",
      "Epoch 3/100\n",
      "262/262 [==============================] - 7s 26ms/step - loss: 375.6496 - val_loss: 28.3484\n",
      "Epoch 4/100\n",
      "262/262 [==============================] - 6s 22ms/step - loss: 61.8395 - val_loss: 25.5368\n",
      "Epoch 5/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 51.1828 - val_loss: 22.1060\n",
      "Epoch 6/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 46.6485 - val_loss: 31.8160\n",
      "Epoch 7/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 42.2375 - val_loss: 17.4177\n",
      "Epoch 8/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 40.0419 - val_loss: 20.5892\n",
      "Epoch 9/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 38.1864 - val_loss: 15.8935\n",
      "Epoch 10/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 36.0414 - val_loss: 20.1216\n",
      "Epoch 11/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 35.2714 - val_loss: 14.7104\n",
      "Epoch 12/100\n",
      "262/262 [==============================] - 5s 18ms/step - loss: 32.4919 - val_loss: 14.8471\n",
      "Epoch 13/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 32.9472 - val_loss: 20.9419\n",
      "Epoch 14/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 30.2218 - val_loss: 15.3573\n",
      "Epoch 15/100\n",
      "262/262 [==============================] - 7s 28ms/step - loss: 30.5955 - val_loss: 14.2100\n",
      "Epoch 16/100\n",
      "262/262 [==============================] - 7s 25ms/step - loss: 31.2692 - val_loss: 15.0516\n",
      "Epoch 17/100\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 28.0811 - val_loss: 13.7453\n",
      "Epoch 18/100\n",
      "262/262 [==============================] - 5s 21ms/step - loss: 29.5187 - val_loss: 14.2510\n",
      "Epoch 19/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 29.0101 - val_loss: 22.1613\n",
      "Epoch 20/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 28.2712 - val_loss: 13.3452\n",
      "Epoch 21/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 28.9265 - val_loss: 13.3805\n",
      "Epoch 22/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 29.8913 - val_loss: 15.1681\n",
      "Epoch 23/100\n",
      "262/262 [==============================] - 5s 21ms/step - loss: 27.8941 - val_loss: 14.4008\n",
      "Epoch 24/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 27.5472 - val_loss: 13.6150\n",
      "Epoch 25/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 32.2568 - val_loss: 13.3629\n",
      "Epoch 26/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.7532 - val_loss: 14.4180\n",
      "Epoch 27/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 27.0647 - val_loss: 14.7896\n",
      "Epoch 28/100\n",
      "262/262 [==============================] - 5s 21ms/step - loss: 26.4740 - val_loss: 13.3783\n",
      "Epoch 29/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 26.1446 - val_loss: 14.2068\n",
      "Epoch 30/100\n",
      "262/262 [==============================] - 6s 21ms/step - loss: 25.9158 - val_loss: 13.7316\n",
      "Epoch 31/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.3402 - val_loss: 14.0169\n",
      "Epoch 32/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 26.9878 - val_loss: 15.3424\n",
      "Epoch 33/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 27.7675 - val_loss: 13.2144\n",
      "Epoch 34/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.1128 - val_loss: 18.3952\n",
      "Epoch 35/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.9745 - val_loss: 13.7429\n",
      "Epoch 36/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.7301 - val_loss: 14.1857\n",
      "Epoch 37/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 28.0191 - val_loss: 14.4966\n",
      "Epoch 38/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.2724 - val_loss: 13.0411\n",
      "Epoch 39/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.7557 - val_loss: 14.2273\n",
      "Epoch 40/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.4332 - val_loss: 14.5626\n",
      "Epoch 41/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 26.9706 - val_loss: 16.8499\n",
      "Epoch 42/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.2714 - val_loss: 15.5592\n",
      "Epoch 43/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.8578 - val_loss: 17.3715\n",
      "Epoch 44/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.8924 - val_loss: 13.9636\n",
      "Epoch 45/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.5700 - val_loss: 16.0978\n",
      "Epoch 46/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 24.9796 - val_loss: 14.2094\n",
      "Epoch 47/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.7428 - val_loss: 16.4968\n",
      "Epoch 48/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 25.5224 - val_loss: 13.9288\n",
      "Epoch 49/100\n",
      "262/262 [==============================] - 5s 18ms/step - loss: 25.9722 - val_loss: 14.8283\n",
      "Epoch 50/100\n",
      "262/262 [==============================] - 5s 18ms/step - loss: 24.9256 - val_loss: 16.5119\n",
      "Epoch 51/100\n",
      "262/262 [==============================] - 5s 18ms/step - loss: 25.0220 - val_loss: 14.2953\n",
      "Epoch 52/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 24.1853 - val_loss: 15.1156\n",
      "Epoch 53/100\n",
      "262/262 [==============================] - 5s 18ms/step - loss: 26.7121 - val_loss: 14.8152\n",
      "Epoch 54/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 27.2000 - val_loss: 13.4509\n",
      "Epoch 55/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 24.4269 - val_loss: 13.4543\n",
      "Epoch 56/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 24.7403 - val_loss: 13.7636\n",
      "Epoch 57/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 127.4097 - val_loss: 15.7816\n",
      "Epoch 58/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 29.8167 - val_loss: 15.1266\n",
      "Epoch 59/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 29.0343 - val_loss: 18.1189\n",
      "Epoch 60/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 28.2757 - val_loss: 14.4773\n",
      "Epoch 61/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 28.3994 - val_loss: 13.9748\n",
      "Epoch 62/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.5037 - val_loss: 14.3944\n",
      "Epoch 63/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.3188 - val_loss: 13.3899\n",
      "Epoch 64/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 24.8262 - val_loss: 15.1221\n",
      "Epoch 65/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.5017 - val_loss: 4567.9038\n",
      "Epoch 66/100\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 63.7229 - val_loss: 13.7158\n",
      "Epoch 67/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 27.8723 - val_loss: 14.1606\n",
      "Epoch 68/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.0567 - val_loss: 13.6797\n",
      "Epoch 69/100\n",
      "262/262 [==============================] - 5s 21ms/step - loss: 25.8300 - val_loss: 18.8639\n",
      "Epoch 70/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.0109 - val_loss: 14.9756\n",
      "Epoch 71/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.7446 - val_loss: 13.6827\n",
      "Epoch 72/100\n",
      "262/262 [==============================] - 6s 22ms/step - loss: 25.2588 - val_loss: 14.0372\n",
      "Epoch 73/100\n",
      "262/262 [==============================] - 6s 21ms/step - loss: 60.0832 - val_loss: 16.2243\n",
      "Epoch 74/100\n",
      "262/262 [==============================] - 6s 22ms/step - loss: 28.9866 - val_loss: 18.6528\n",
      "Epoch 75/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 29.4774 - val_loss: 16.0974\n",
      "Epoch 76/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 28.5452 - val_loss: 14.8101\n",
      "Epoch 77/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 27.8891 - val_loss: 15.0250\n",
      "Epoch 78/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 27.2143 - val_loss: 14.9772\n",
      "Epoch 79/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 27.8477 - val_loss: 19.1783\n",
      "Epoch 80/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 28.0119 - val_loss: 15.1667\n",
      "Epoch 81/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 27.7640 - val_loss: 15.0558\n",
      "Epoch 82/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 26.7858 - val_loss: 15.5603\n",
      "Epoch 83/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 26.7508 - val_loss: 14.3402\n",
      "Epoch 84/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 26.5366 - val_loss: 18.2021\n",
      "Epoch 85/100\n",
      "262/262 [==============================] - 5s 21ms/step - loss: 26.2615 - val_loss: 15.7421\n",
      "Epoch 86/100\n",
      "262/262 [==============================] - 6s 23ms/step - loss: 27.2336 - val_loss: 15.8436\n",
      "Epoch 87/100\n",
      "262/262 [==============================] - 5s 20ms/step - loss: 25.4891 - val_loss: 23.4334\n",
      "Epoch 88/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 25.2452 - val_loss: 15.6338\n",
      "Epoch 89/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 25.8470 - val_loss: 15.9953\n",
      "Epoch 90/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 25.5393 - val_loss: 16.2008\n",
      "Epoch 91/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 25.3732 - val_loss: 13.7305\n",
      "Epoch 92/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 26.5208 - val_loss: 13.8264\n",
      "Epoch 93/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 25.4436 - val_loss: 18.5838\n",
      "Epoch 94/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 25.7472 - val_loss: 15.7695\n",
      "Epoch 95/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 25.1069 - val_loss: 16.9639\n",
      "Epoch 96/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 26.0722 - val_loss: 14.0541\n",
      "Epoch 97/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 25.7753 - val_loss: 15.1213\n",
      "Epoch 98/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 24.5895 - val_loss: 14.3661\n",
      "Epoch 99/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 24.4719 - val_loss: 13.6189\n",
      "Epoch 100/100\n",
      "262/262 [==============================] - 5s 19ms/step - loss: 24.8330 - val_loss: 17.4231\n"
     ]
    }
   ],
   "source": [
    "lstm2 =LSTM2()\n",
    "train_X, train_y, test_X, test_y = get_data()\n",
    "lstm2.train(train_X, train_y, test_X, test_y)\n",
    "lstm2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 1s 8ms/step\n",
      "R2 score: 0.9847401230687274\n",
      "RMSE:  3.6112506484232014\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = get_data()\n",
    "r2, rmse, y_preds = lstm2.evaluate(test_X = test_X, test_y = test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, MaxPooling1D, Flatten, Conv1D, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "class LSTM2_multistep():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.model_root = 'models/'\n",
    "        self.model_name = 'LSTM2_multistep.h5'\n",
    "        self.model_path = self.model_root + self.model_name\n",
    "        \n",
    "    def build_model(self, in_steps, out_steps):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, activation='relu', return_sequences= True, input_shape = (in_steps, 1)))\n",
    "        model.add(LSTM(32, activation='relu'))\n",
    "        model.add(Dense(out_steps))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        self.model = model\n",
    "        print(\"MLP moedel built!\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self,train_X, train_y, test_X, test_y, verbose = 1, epochs = 100,in_steps = 27, out_steps = 7):\n",
    "        if self.model == None:\n",
    "          model =  self.build_model(in_steps= in_steps, out_steps= out_steps)\n",
    "        checkpoint = ModelCheckpoint(self.model_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "        model.fit(train_X, train_y, epochs = epochs, verbose = verbose, validation_data = (test_X, test_y), callbacks=[checkpoint])\n",
    "        self.model = model\n",
    "          \n",
    "    def evaluate(self, test_X, test_y):\n",
    "        model = self.model\n",
    "        model.load_weights(self.model_path)\n",
    "        # Assuming you have your true values in 'y_true' and predicted values in 'y_pred'\n",
    "        pred_y = model.predict(test_X)\n",
    "        RMSE_scores = []\n",
    "        R2_scores = []    \n",
    "        for i in range(test_y.shape[1]):\n",
    "            mse = mean_squared_error(test_y[:, i], pred_y[:,  i])\n",
    "            rmse = np.sqrt(mse)\n",
    "            RMSE_scores.append(rmse)\n",
    "            \n",
    "        for i in range(test_y.shape[1]):\n",
    "            r2 = r2_score(test_y[:, i], pred_y[:,  i])        \n",
    "            R2_scores.append(r2)\n",
    "            \n",
    "        total_score = 0\n",
    "        for row in range(test_y.shape[0]):\n",
    "            for col in range(pred_y.shape[1]):\n",
    "                total_score += (test_y[row, col] - pred_y[row, col]**2)\n",
    "                total_score = np.sqrt(total_score/(test_y.shape[0] * test_y.shape[1]))\n",
    "                \n",
    "        return RMSE_scores, R2_scores, total_score, pred_y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP moedel built!\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/261 [==============================] - 8s 23ms/step - loss: 12687.4756 - val_loss: 438.3090\n",
      "Epoch 2/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 1434.5106 - val_loss: 164.8703\n",
      "Epoch 3/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 534.3317 - val_loss: 147.4663\n",
      "Epoch 4/100\n",
      "261/261 [==============================] - 6s 23ms/step - loss: 419.2549 - val_loss: 130.7292\n",
      "Epoch 5/100\n",
      "261/261 [==============================] - 6s 23ms/step - loss: 382.9825 - val_loss: 126.1501\n",
      "Epoch 6/100\n",
      "261/261 [==============================] - 6s 24ms/step - loss: 348.6004 - val_loss: 120.8141\n",
      "Epoch 7/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 332.3812 - val_loss: 115.5973\n",
      "Epoch 8/100\n",
      "261/261 [==============================] - 6s 23ms/step - loss: 322.8432 - val_loss: 112.7555\n",
      "Epoch 9/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 315.3603 - val_loss: 112.0321\n",
      "Epoch 10/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 306.3860 - val_loss: 112.7968\n",
      "Epoch 11/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 301.0919 - val_loss: 108.5506\n",
      "Epoch 12/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 294.9803 - val_loss: 110.2968\n",
      "Epoch 13/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 290.2737 - val_loss: 108.3100\n",
      "Epoch 14/100\n",
      "261/261 [==============================] - 5s 21ms/step - loss: 284.0563 - val_loss: 121.2084\n",
      "Epoch 15/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 277.1612 - val_loss: 109.1496\n",
      "Epoch 16/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 273.2074 - val_loss: 99.0460\n",
      "Epoch 17/100\n",
      "261/261 [==============================] - 5s 21ms/step - loss: 263.3201 - val_loss: 90.9408\n",
      "Epoch 18/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 250.8442 - val_loss: 92.3791\n",
      "Epoch 19/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 247.4200 - val_loss: 92.5735\n",
      "Epoch 20/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 244.7489 - val_loss: 91.0379\n",
      "Epoch 21/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 240.5942 - val_loss: 93.9443\n",
      "Epoch 22/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 240.5237 - val_loss: 90.8964\n",
      "Epoch 23/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 236.4661 - val_loss: 97.8855\n",
      "Epoch 24/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 234.6015 - val_loss: 92.3245\n",
      "Epoch 25/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 233.3509 - val_loss: 86.4386\n",
      "Epoch 26/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 243.7025 - val_loss: 103.0956\n",
      "Epoch 27/100\n",
      "261/261 [==============================] - 6s 23ms/step - loss: 232.6996 - val_loss: 89.4755\n",
      "Epoch 28/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 231.8163 - val_loss: 91.0776\n",
      "Epoch 29/100\n",
      "261/261 [==============================] - 6s 24ms/step - loss: 233.5904 - val_loss: 90.6751\n",
      "Epoch 30/100\n",
      "261/261 [==============================] - 6s 23ms/step - loss: 229.6293 - val_loss: 84.2649\n",
      "Epoch 31/100\n",
      "261/261 [==============================] - 6s 25ms/step - loss: 225.8690 - val_loss: 98.8756\n",
      "Epoch 32/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 230.3902 - val_loss: 91.0444\n",
      "Epoch 33/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 226.6258 - val_loss: 83.7961\n",
      "Epoch 34/100\n",
      "261/261 [==============================] - 7s 27ms/step - loss: 230.2549 - val_loss: 86.2323\n",
      "Epoch 35/100\n",
      "261/261 [==============================] - 7s 25ms/step - loss: 226.3678 - val_loss: 84.8658\n",
      "Epoch 36/100\n",
      "261/261 [==============================] - 6s 23ms/step - loss: 228.7462 - val_loss: 101.3909\n",
      "Epoch 37/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 229.2603 - val_loss: 88.6519\n",
      "Epoch 38/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 227.4528 - val_loss: 86.2258\n",
      "Epoch 39/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 228.7880 - val_loss: 95.3239\n",
      "Epoch 40/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 225.0384 - val_loss: 104.9066\n",
      "Epoch 41/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 221.4019 - val_loss: 88.5618\n",
      "Epoch 42/100\n",
      "261/261 [==============================] - 5s 21ms/step - loss: 219.6311 - val_loss: 85.7443\n",
      "Epoch 43/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 227.8817 - val_loss: 103.2211\n",
      "Epoch 44/100\n",
      "261/261 [==============================] - 5s 21ms/step - loss: 225.7890 - val_loss: 87.3305\n",
      "Epoch 45/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 235.3308 - val_loss: 86.6195\n",
      "Epoch 46/100\n",
      "261/261 [==============================] - 5s 21ms/step - loss: 224.7025 - val_loss: 83.9855\n",
      "Epoch 47/100\n",
      "261/261 [==============================] - 5s 21ms/step - loss: 224.7834 - val_loss: 98.3557\n",
      "Epoch 48/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 223.0115 - val_loss: 89.9677\n",
      "Epoch 49/100\n",
      "261/261 [==============================] - 5s 21ms/step - loss: 225.0680 - val_loss: 92.2750\n",
      "Epoch 50/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 223.0466 - val_loss: 90.2109\n",
      "Epoch 51/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 231.3621 - val_loss: 87.6086\n",
      "Epoch 52/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 229.1117 - val_loss: 131.9990\n",
      "Epoch 53/100\n",
      "261/261 [==============================] - 5s 21ms/step - loss: 228.4133 - val_loss: 99.7258\n",
      "Epoch 54/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 218.3453 - val_loss: 86.6135\n",
      "Epoch 55/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 218.9101 - val_loss: 107.7799\n",
      "Epoch 56/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 233.2504 - val_loss: 85.2759\n",
      "Epoch 57/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 229.3765 - val_loss: 85.9058\n",
      "Epoch 58/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 252.0555 - val_loss: 88.0888\n",
      "Epoch 59/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 224.3351 - val_loss: 81.6213\n",
      "Epoch 60/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 220.9503 - val_loss: 109.2566\n",
      "Epoch 61/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 223.8024 - val_loss: 85.5525\n",
      "Epoch 62/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 220.2165 - val_loss: 82.5295\n",
      "Epoch 63/100\n",
      "261/261 [==============================] - 6s 21ms/step - loss: 218.9395 - val_loss: 88.5189\n",
      "Epoch 64/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 220.3026 - val_loss: 83.3673\n",
      "Epoch 65/100\n",
      "261/261 [==============================] - 6s 22ms/step - loss: 215.4935 - val_loss: 82.4777\n",
      "Epoch 66/100\n",
      " 86/261 [========>.....................] - ETA: 3s - loss: 207.1815"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Swaroop\\solarflux predictor\\models.ipynb Cell 19\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m LSTM2_multistep \u001b[39m=\u001b[39m LSTM2_multistep()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_X, train_y, test_X, test_y \u001b[39m=\u001b[39m get_data_multistep(\u001b[39m27\u001b[39m,\u001b[39m7\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m LSTM2_multistep\u001b[39m.\u001b[39;49mtrain(train_X, train_y, test_X, test_y, in_steps\u001b[39m=\u001b[39;49m \u001b[39m27\u001b[39;49m, out_steps\u001b[39m=\u001b[39;49m \u001b[39m7\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Swaroop\\solarflux predictor\\models.ipynb Cell 19\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X34sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m   model \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_model(in_steps\u001b[39m=\u001b[39m in_steps, out_steps\u001b[39m=\u001b[39m out_steps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X34sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_path, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X34sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_X, train_y, epochs \u001b[39m=\u001b[39;49m epochs, verbose \u001b[39m=\u001b[39;49m verbose, validation_data \u001b[39m=\u001b[39;49m (test_X, test_y), callbacks\u001b[39m=\u001b[39;49m[checkpoint])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Swaroop/solarflux%20predictor/models.ipynb#X34sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LSTM2_multistep = LSTM2_multistep()\n",
    "train_X, train_y, test_X, test_y = get_data_multistep(27,7)\n",
    "LSTM2_multistep.train(train_X, train_y, test_X, test_y, in_steps= 27, out_steps= 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 1s 7ms/step\n",
      "R2 score: [5.569040744042218, 6.077633658596724, 7.230882510524091, 8.752351749838269, 10.021950797778777, 11.187511938800254, 12.20278257374658]\n",
      "RMSE:  [0.9625902376094795, 0.9554702454266346, 0.9370016270274607, 0.9077516133247489, 0.8791108803496764, 0.8494193047894008, 0.8209147594804774]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Swaroop\\AppData\\Local\\Temp\\ipykernel_3412\\689974487.py:57: RuntimeWarning: invalid value encountered in sqrt\n",
      "  total_score = np.sqrt(total_score/(test_y.shape[0] * test_y.shape[1]))\n"
     ]
    }
   ],
   "source": [
    "# train_X, train_y, test_X, test_y = get_data_multistep(27,7)\n",
    "r2, rmse, total_score, y_preds = LSTM2_multistep.evaluate(test_X = test_X, test_y = test_y)\n",
    "print(\"R2 score:\", r2)\n",
    "print(\"RMSE: \", rmse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, LeakyReLU, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class LSTM3():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.model_root = 'models/'\n",
    "        self.model_name = 'LSTM3.h5'\n",
    "        self.model_path = self.model_root + self.model_name\n",
    "        \n",
    "    def build_model(self, lag):\n",
    "        # Define model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128,return_sequences= True,  input_shape=(27,1)))\n",
    "        model.add(LeakyReLU(alpha = 0.5))\n",
    "        model.add(LSTM(128, return_sequences=True))\n",
    "        model.add(LeakyReLU(alpha = 0.5))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(LSTM(64))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        self.model = model\n",
    "        print(\"MLP moedel built!\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self,train_X, train_y, test_X, test_y, verbose = 1, epochs = 100, lag = 27):\n",
    "        if self.model == None:\n",
    "          model =  self.build_model(lag)\n",
    "        checkpoint = ModelCheckpoint(self.model_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "        model.fit(train_X, train_y, epochs = epochs, verbose = verbose, validation_data = (test_X, test_y), callbacks=[checkpoint])\n",
    "        self.model = model\n",
    "          \n",
    "    def evaluate(self, test_X, test_y):\n",
    "        model = self.model\n",
    "        model.load_weights(self.model_path)\n",
    "        # Assuming you have your true values in 'y_true' and predicted values in 'y_pred'\n",
    "        pred_y = model.predict(test_X)\n",
    "        r2 = r2_score(test_y, pred_y)\n",
    "        rmse = np.sqrt(mean_squared_error(test_y, pred_y))\n",
    "        print(\"R2 score:\", r2)\n",
    "        print(\"RMSE: \", rmse)        \n",
    "        return r2, rmse, pred_y\n",
    "\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm3 =LSTM3()\n",
    "train_X, train_y, test_X, test_y = get_data()\n",
    "lstm3.train(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 3s 17ms/step\n",
      "R2 score: -0.002549201229657516\n",
      "RMSE:  29.270825115073862\n"
     ]
    }
   ],
   "source": [
    "r2, rmse, y_preds = lstm3.evaluate(test_X = test_X, test_y = test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, LeakyReLU, Dropout, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class LSTMCNN():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.model_root = 'models/'\n",
    "        self.model_name = 'CNNLSTM2.h5'\n",
    "        self.model_path = self.model_root + self.model_name\n",
    "        \n",
    "    def build_model(self, lag):\n",
    "        # Define model\n",
    "        \n",
    "        model_cnn_lstm = Sequential()\n",
    "        model_cnn_lstm.add(TimeDistributed(Conv1D(filters=16, kernel_size=14, activation='relu'), input_shape=(None, 27, 1)))\n",
    "        model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        model_cnn_lstm.add(TimeDistributed(Conv1D(filters=32, kernel_size=4, activation='relu')))\n",
    "        model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "        model_cnn_lstm.add(Dense(128))\n",
    "        model_cnn_lstm.add(LSTM(64, activation='relu', return_sequences= True))\n",
    "        model_cnn_lstm.add(LSTM(32, activation='relu'))\n",
    "        model_cnn_lstm.add(Dense(1))\n",
    "        model_cnn_lstm.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "    # Print the model summary\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model_cnn_lstm.compile(optimizer=optimizer, loss='mse')\n",
    "        self.model = model_cnn_lstm\n",
    "        print(\"moedel built!\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self,train_X, train_y, test_X, test_y, verbose = 1, epochs = 100, lag = 27):\n",
    "        if self.model == None:\n",
    "          model =  self.build_model(lag)\n",
    "        checkpoint = ModelCheckpoint(self.model_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "        model.fit(train_X, train_y, epochs = epochs, verbose = verbose, validation_data = (test_X, test_y), callbacks=[checkpoint])\n",
    "        self.model = model\n",
    "          \n",
    "    def evaluate(self, test_X, test_y):\n",
    "        model = self.model\n",
    "        model.load_weights(self.model_path)\n",
    "        # Assuming you have your true values in 'y_true' and predicted values in 'y_pred'\n",
    "        pred_y = model.predict(test_X)\n",
    "        r2 = r2_score(test_y, pred_y)\n",
    "        rmse = np.sqrt(mean_squared_error(test_y, pred_y))\n",
    "        print(\"R2 score:\", r2)\n",
    "        print(\"RMSE: \", rmse)        \n",
    "        return r2, rmse, pred_y\n",
    "\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moedel built!\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/262 [==============================] - 4s 6ms/step - loss: 1653.9038 - val_loss: 120.5342\n",
      "Epoch 2/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 304.8667 - val_loss: 103.3963\n",
      "Epoch 3/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 229.5621 - val_loss: 65.5013\n",
      "Epoch 4/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 184.1823 - val_loss: 79.6671\n",
      "Epoch 5/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 160.2560 - val_loss: 54.1204\n",
      "Epoch 6/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 141.9005 - val_loss: 42.4044\n",
      "Epoch 7/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 119.3657 - val_loss: 48.7666\n",
      "Epoch 8/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 108.2964 - val_loss: 39.0499\n",
      "Epoch 9/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 99.8905 - val_loss: 35.0641\n",
      "Epoch 10/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 98.4489 - val_loss: 40.2214\n",
      "Epoch 11/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 90.2143 - val_loss: 40.5616\n",
      "Epoch 12/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 82.1861 - val_loss: 41.9174\n",
      "Epoch 13/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 78.6166 - val_loss: 35.6237\n",
      "Epoch 14/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 81.6090 - val_loss: 29.0908\n",
      "Epoch 15/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 69.3013 - val_loss: 32.6408\n",
      "Epoch 16/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 69.4605 - val_loss: 29.0827\n",
      "Epoch 17/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 69.3228 - val_loss: 29.5889\n",
      "Epoch 18/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 75.1706 - val_loss: 27.8982\n",
      "Epoch 19/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 64.5157 - val_loss: 26.7326\n",
      "Epoch 20/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 71.1408 - val_loss: 29.8904\n",
      "Epoch 21/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 62.5965 - val_loss: 27.1170\n",
      "Epoch 22/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 58.0785 - val_loss: 34.9137\n",
      "Epoch 23/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 59.2208 - val_loss: 26.7211\n",
      "Epoch 24/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 56.3529 - val_loss: 25.9335\n",
      "Epoch 25/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 56.3249 - val_loss: 24.4934\n",
      "Epoch 26/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 58.3732 - val_loss: 27.9477\n",
      "Epoch 27/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 52.3425 - val_loss: 23.5549\n",
      "Epoch 28/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 52.1527 - val_loss: 26.1519\n",
      "Epoch 29/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 52.5420 - val_loss: 22.1363\n",
      "Epoch 30/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 51.2139 - val_loss: 23.8189\n",
      "Epoch 31/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 52.7052 - val_loss: 23.0130\n",
      "Epoch 32/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 49.4636 - val_loss: 24.7735\n",
      "Epoch 33/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 45.6431 - val_loss: 21.6190\n",
      "Epoch 34/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 49.8014 - val_loss: 21.1958\n",
      "Epoch 35/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 43.9267 - val_loss: 21.7988\n",
      "Epoch 36/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 52.7097 - val_loss: 22.9716\n",
      "Epoch 37/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 46.6308 - val_loss: 28.2208\n",
      "Epoch 38/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 47.2385 - val_loss: 26.0560\n",
      "Epoch 39/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 44.4799 - val_loss: 20.2063\n",
      "Epoch 40/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 43.5996 - val_loss: 21.6458\n",
      "Epoch 41/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 46.7751 - val_loss: 20.1933\n",
      "Epoch 42/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 46.6042 - val_loss: 22.6264\n",
      "Epoch 43/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 43.0874 - val_loss: 26.5716\n",
      "Epoch 44/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 43.9603 - val_loss: 20.0627\n",
      "Epoch 45/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 42.5551 - val_loss: 19.5260\n",
      "Epoch 46/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 44.9649 - val_loss: 19.1385\n",
      "Epoch 47/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 43.0196 - val_loss: 19.7649\n",
      "Epoch 48/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 41.2002 - val_loss: 20.0173\n",
      "Epoch 49/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 42.5152 - val_loss: 22.2540\n",
      "Epoch 50/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 46.3416 - val_loss: 22.0052\n",
      "Epoch 51/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 40.0221 - val_loss: 20.5351\n",
      "Epoch 52/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 41.9564 - val_loss: 19.8003\n",
      "Epoch 53/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 42.1621 - val_loss: 19.1420\n",
      "Epoch 54/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 39.1219 - val_loss: 24.1218\n",
      "Epoch 55/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 40.9040 - val_loss: 21.3774\n",
      "Epoch 56/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 40.0191 - val_loss: 18.9023\n",
      "Epoch 57/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.1558 - val_loss: 21.7065\n",
      "Epoch 58/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 41.8915 - val_loss: 23.1816\n",
      "Epoch 59/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 38.7252 - val_loss: 21.6683\n",
      "Epoch 60/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.7934 - val_loss: 24.2416\n",
      "Epoch 61/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 38.8614 - val_loss: 18.6223\n",
      "Epoch 62/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.8381 - val_loss: 18.9204\n",
      "Epoch 63/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 40.4580 - val_loss: 22.6957\n",
      "Epoch 64/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 39.9406 - val_loss: 22.5032\n",
      "Epoch 65/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 36.9395 - val_loss: 18.7299\n",
      "Epoch 66/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 36.9069 - val_loss: 20.9278\n",
      "Epoch 67/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 39.3209 - val_loss: 19.2524\n",
      "Epoch 68/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 36.0713 - val_loss: 18.6911\n",
      "Epoch 69/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.9335 - val_loss: 21.6191\n",
      "Epoch 70/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.6615 - val_loss: 19.2177\n",
      "Epoch 71/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 38.9894 - val_loss: 18.7298\n",
      "Epoch 72/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 36.6908 - val_loss: 22.0382\n",
      "Epoch 73/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 36.4427 - val_loss: 20.7376\n",
      "Epoch 74/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 35.8647 - val_loss: 18.7825\n",
      "Epoch 75/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 38.2683 - val_loss: 19.3448\n",
      "Epoch 76/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 42.6522 - val_loss: 19.0870\n",
      "Epoch 77/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 36.2717 - val_loss: 18.5835\n",
      "Epoch 78/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.8131 - val_loss: 22.3161\n",
      "Epoch 79/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.9695 - val_loss: 23.9990\n",
      "Epoch 80/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.4675 - val_loss: 29.8565\n",
      "Epoch 81/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 35.1042 - val_loss: 18.9104\n",
      "Epoch 82/100\n",
      "262/262 [==============================] - 1s 5ms/step - loss: 34.4860 - val_loss: 17.7539\n",
      "Epoch 83/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 40.3908 - val_loss: 19.8134\n",
      "Epoch 84/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.3272 - val_loss: 25.5223\n",
      "Epoch 85/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 36.1716 - val_loss: 25.6643\n",
      "Epoch 86/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 34.1779 - val_loss: 18.6590\n",
      "Epoch 87/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 36.2620 - val_loss: 23.4162\n",
      "Epoch 88/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 31.7458 - val_loss: 20.7797\n",
      "Epoch 89/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 35.2267 - val_loss: 19.9519\n",
      "Epoch 90/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 34.4890 - val_loss: 18.8511\n",
      "Epoch 91/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 35.4213 - val_loss: 21.0848\n",
      "Epoch 92/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 38.3941 - val_loss: 19.6170\n",
      "Epoch 93/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 33.8539 - val_loss: 18.8299\n",
      "Epoch 94/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 34.9463 - val_loss: 24.3645\n",
      "Epoch 95/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 33.7225 - val_loss: 27.5638\n",
      "Epoch 96/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 33.9002 - val_loss: 21.7091\n",
      "Epoch 97/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 33.6561 - val_loss: 21.7738\n",
      "Epoch 98/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 37.0172 - val_loss: 23.1615\n",
      "Epoch 99/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 33.8777 - val_loss: 20.7537\n",
      "Epoch 100/100\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 32.1871 - val_loss: 18.9793\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn =LSTMCNN()\n",
    "train_X, train_y, test_X, test_y = get_data_CNN()\n",
    "lstm_cnn.train(train_X, train_y, test_X, test_y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 1s 2ms/step\n",
      "R2 score: 0.9792255153486535\n",
      "RMSE:  4.21354121973101\n"
     ]
    }
   ],
   "source": [
    "r2, rmse, y_preds = lstm_cnn.evaluate(test_X = test_X, test_y = test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, LeakyReLU, Dropout, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class LSTMCNN_multistep():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.model_root = 'models/'\n",
    "        self.model_name = 'CNNLSTM2_multisteps.h5'\n",
    "        self.model_path = self.model_root + self.model_name\n",
    "        \n",
    "    def build_model(self, in_steps, out_steps):\n",
    "        # Define model\n",
    "        \n",
    "        model_cnn_lstm = Sequential()\n",
    "        model_cnn_lstm.add(TimeDistributed(Conv1D(filters=16, kernel_size=14, activation='relu'), input_shape=(None, in_steps, 1)))\n",
    "        model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        model_cnn_lstm.add(TimeDistributed(Conv1D(filters=32, kernel_size=4, activation='relu')))\n",
    "        model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "        model_cnn_lstm.add(Dense(128))\n",
    "        model_cnn_lstm.add(LSTM(64, activation='relu', return_sequences= True))\n",
    "        model_cnn_lstm.add(LSTM(32, activation='relu'))\n",
    "        model_cnn_lstm.add(Dense(out_steps))\n",
    "        model_cnn_lstm.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "    # Print the model summary\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model_cnn_lstm.compile(optimizer=optimizer, loss='mse')\n",
    "        self.model = model_cnn_lstm\n",
    "        print(\"moedel built!\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self,train_X, train_y, test_X, test_y, verbose = 1, epochs = 100, in_steps = 27, out_steps = 3):\n",
    "        if self.model == None:\n",
    "          model =  self.build_model(in_steps= in_steps, out_steps= out_steps)\n",
    "        checkpoint = ModelCheckpoint(self.model_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "        model.fit(train_X, train_y, epochs = epochs, verbose = verbose, validation_data = (test_X, test_y), callbacks=[checkpoint])\n",
    "        self.model = model\n",
    "          \n",
    "    def evaluate(self, test_X, test_y):\n",
    "        model = self.model\n",
    "        model.load_weights(self.model_path)\n",
    "        # Assuming you have your true values in 'y_true' and predicted values in 'y_pred'\n",
    "        pred_y = model.predict(test_X)\n",
    "        RMSE_scores = []\n",
    "        R2_scores = []    \n",
    "        for i in range(test_y.shape[1]):\n",
    "            mse = mean_squared_error(test_y[:, i], pred_y[:,  i])\n",
    "            rmse = np.sqrt(mse)\n",
    "            RMSE_scores.append(rmse)\n",
    "            \n",
    "        for i in range(test_y.shape[1]):\n",
    "            r2 = r2_score(test_y[:, i], pred_y[:,  i])        \n",
    "            R2_scores.append(r2)\n",
    "            \n",
    "        total_score = 0\n",
    "        for row in range(test_y.shape[0]):\n",
    "            for col in range(pred_y.shape[1]):\n",
    "                total_score += (test_y[row, col] - pred_y[row, col]**2)\n",
    "                total_score = np.sqrt(total_score/(test_y.shape[0] * test_y.shape[1]))\n",
    "                \n",
    "        return RMSE_scores, R2_scores, total_score, pred_y\n",
    "\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moedel built!\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Swaroop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/261 [==============================] - 4s 6ms/step - loss: 2731.1006 - val_loss: 178.7089\n",
      "Epoch 2/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 471.0459 - val_loss: 159.4430\n",
      "Epoch 3/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 419.3045 - val_loss: 133.4439\n",
      "Epoch 4/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 383.2812 - val_loss: 146.6740\n",
      "Epoch 5/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 353.6266 - val_loss: 131.3352\n",
      "Epoch 6/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 335.9055 - val_loss: 110.3918\n",
      "Epoch 7/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 320.8566 - val_loss: 116.2676\n",
      "Epoch 8/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 328.5031 - val_loss: 114.8729\n",
      "Epoch 9/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 314.5367 - val_loss: 116.7516\n",
      "Epoch 10/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 310.7917 - val_loss: 105.1248\n",
      "Epoch 11/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 300.3306 - val_loss: 100.0336\n",
      "Epoch 12/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 295.5184 - val_loss: 151.9483\n",
      "Epoch 13/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 400.4660 - val_loss: 109.8326\n",
      "Epoch 14/60\n",
      "261/261 [==============================] - 1s 4ms/step - loss: 319.6023 - val_loss: 108.5503\n",
      "Epoch 15/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 313.1564 - val_loss: 112.0790\n",
      "Epoch 16/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 306.1018 - val_loss: 111.2532\n",
      "Epoch 17/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 299.8130 - val_loss: 108.3791\n",
      "Epoch 18/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 298.0607 - val_loss: 116.2386\n",
      "Epoch 19/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 295.6714 - val_loss: 104.8712\n",
      "Epoch 20/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 289.4446 - val_loss: 111.2872\n",
      "Epoch 21/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 297.2573 - val_loss: 109.8104\n",
      "Epoch 22/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 283.1578 - val_loss: 101.5942\n",
      "Epoch 23/60\n",
      "261/261 [==============================] - 1s 4ms/step - loss: 286.2426 - val_loss: 110.8711\n",
      "Epoch 24/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 287.0446 - val_loss: 96.8143\n",
      "Epoch 25/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 280.0869 - val_loss: 97.6262\n",
      "Epoch 26/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 271.6897 - val_loss: 96.4873\n",
      "Epoch 27/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 275.6575 - val_loss: 95.2161\n",
      "Epoch 28/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 265.4727 - val_loss: 95.9907\n",
      "Epoch 29/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 264.4840 - val_loss: 93.2921\n",
      "Epoch 30/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 262.0799 - val_loss: 92.2476\n",
      "Epoch 31/60\n",
      "261/261 [==============================] - 1s 6ms/step - loss: 261.5561 - val_loss: 96.4602\n",
      "Epoch 32/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 256.0678 - val_loss: 121.5342\n",
      "Epoch 33/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 260.5504 - val_loss: 92.2097\n",
      "Epoch 34/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 260.8303 - val_loss: 92.7602\n",
      "Epoch 35/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 254.5649 - val_loss: 92.9418\n",
      "Epoch 36/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 258.2799 - val_loss: 107.6859\n",
      "Epoch 37/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 261.8244 - val_loss: 105.2997\n",
      "Epoch 38/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 255.8006 - val_loss: 93.7965\n",
      "Epoch 39/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 254.2105 - val_loss: 96.7813\n",
      "Epoch 40/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 255.3551 - val_loss: 91.5371\n",
      "Epoch 41/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 247.8231 - val_loss: 108.2974\n",
      "Epoch 42/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 246.5430 - val_loss: 155.8144\n",
      "Epoch 43/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 260.6734 - val_loss: 92.1298\n",
      "Epoch 44/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 261.4395 - val_loss: 104.6679\n",
      "Epoch 45/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 258.6235 - val_loss: 119.2069\n",
      "Epoch 46/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 248.8967 - val_loss: 92.4647\n",
      "Epoch 47/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 243.9265 - val_loss: 90.6512\n",
      "Epoch 48/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 248.5244 - val_loss: 109.4585\n",
      "Epoch 49/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 242.5501 - val_loss: 89.1750\n",
      "Epoch 50/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 247.3489 - val_loss: 107.7626\n",
      "Epoch 51/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 243.5612 - val_loss: 89.7760\n",
      "Epoch 52/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 240.1488 - val_loss: 97.0862\n",
      "Epoch 53/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 250.7664 - val_loss: 96.2495\n",
      "Epoch 54/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 238.9135 - val_loss: 89.5196\n",
      "Epoch 55/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 241.8905 - val_loss: 93.6205\n",
      "Epoch 56/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 244.2462 - val_loss: 114.5298\n",
      "Epoch 57/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 241.5377 - val_loss: 105.8115\n",
      "Epoch 58/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 244.1156 - val_loss: 88.4133\n",
      "Epoch 59/60\n",
      "261/261 [==============================] - 1s 4ms/step - loss: 239.1304 - val_loss: 91.9084\n",
      "Epoch 60/60\n",
      "261/261 [==============================] - 1s 5ms/step - loss: 236.4653 - val_loss: 99.4021\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_multistep =LSTMCNN_multistep()\n",
    "train_X, train_y, test_X, test_y = get_CNN_data_multistep(in_steps=27, out_steps=7)\n",
    "lstm_cnn_multistep.train(train_X, train_y, test_X, test_y, epochs=60, in_steps=27, out_steps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2, rmse, y_preds = lstm_cnn-MLP_multistep.evaluate(test_X = test_X, test_y = test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: [5.569040744042218, 6.077633658596724, 7.230882510524091, 8.752351749838269, 10.021950797778777, 11.187511938800254, 12.20278257374658]\n",
      "RMSE:  [0.9625902376094795, 0.9554702454266346, 0.9370016270274607, 0.9077516133247489, 0.8791108803496764, 0.8494193047894008, 0.8209147594804774]\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 score:\", r2)\n",
    "print(\"RMSE: \", rmse)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
